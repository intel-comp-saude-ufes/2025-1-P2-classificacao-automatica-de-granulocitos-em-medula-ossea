{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3ae892b",
   "metadata": {},
   "source": [
    "# Inteligência Computacional em Saúde - Trabalho 02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440119c5",
   "metadata": {},
   "source": [
    "## Carregamento e Pré-Processamentos dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "770cc5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and setup\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from typing import Tuple, List\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from torchvision import models, datasets\n",
    "from torchvision.transforms import v2 as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "277c4b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = './data/bone-marrow-dataset'\n",
    "\n",
    "CHARTS_PATH = \"./charts\"\n",
    "os.makedirs(f\"{CHARTS_PATH}\", exist_ok=True)\n",
    "\n",
    "MODELS_PATH = \"./models\"\n",
    "os.makedirs(f\"{MODELS_PATH}\", exist_ok=True)\n",
    "\n",
    "TARGET_CLASSES = ['ABE', 'BAS', 'BLA', 'EOS', 'FGC', 'MMZ', 'MYB', 'NGB', 'NGS', 'PMO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6608e9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset wrapper that applies transformations to images dynamically.\n",
    "\n",
    "    Args:\n",
    "        data (Dataset): the original dataset\n",
    "        transforms (callable, optional): transformations to apply to each image sample\n",
    "    \"\"\"\n",
    "    def __init__(self, data: Dataset, transforms=None)-> None:\n",
    "        \"\"\"Initialize the TransformedDataset.\"\"\"\n",
    "        self._data = data\n",
    "        self._transforms = transforms\n",
    "        self.classes = self._data.dataset.classes\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, idx: int)-> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"Retrieve a sample from the dataset at the given index, apply transformations if provided.\"\"\"\n",
    "        img, label = self._data[idx]\n",
    "        if self._transforms:\n",
    "            img = self._transforms(img)\n",
    "\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7a827d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = datasets.ImageFolder(root=DATASET_PATH)\n",
    "\n",
    "# extract labels for stratification\n",
    "labels = full_dataset.targets\n",
    "class_names = full_dataset.classes\n",
    "indices = list(range(len(full_dataset)))\n",
    "\n",
    "# first split: 70% for training, 30% for temporary set (val + test)\n",
    "train_indices, temp_indices, train_labels, temp_labels = train_test_split(\n",
    "    indices, labels, test_size=0.3, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# second split: 50/50 for validation and test (15% each)\n",
    "val_indices, test_indices, _, _ = train_test_split(\n",
    "    temp_indices, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
    ")\n",
    "\n",
    "# dataset creation\n",
    "train_subset = Subset(full_dataset, train_indices)\n",
    "val_subset = Subset(full_dataset, val_indices)\n",
    "test_subset = Subset(full_dataset, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8f1d109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation and transformation pipelines\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a3e5123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap the subsets to apply the correct transformations\n",
    "train_dataset = TransformedDataset(train_subset, transforms=train_transforms)\n",
    "val_dataset = TransformedDataset(val_subset, transforms=val_test_transforms)\n",
    "test_dataset = TransformedDataset(test_subset, transforms=val_test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d34aa271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a weighted random sampler to handle class imbalance\n",
    "y_train = torch.tensor(train_labels)\n",
    "class_weights = 1. / torch.bincount(y_train)\n",
    "sampler = WeightedRandomSampler(weights=class_weights[y_train], num_samples=len(class_weights[y_train]), replacement=True)\n",
    "\n",
    "# create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, sampler=sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41c067ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(dataloader: DataLoader, n_images: int = 10) -> None:\n",
    "    \"\"\"Displays a batch of images from a DataLoader.\"\"\"\n",
    "\n",
    "    # get one batch of images and labels from the DataLoader\n",
    "    images, labels = next(iter(dataloader))\n",
    "    class_names = dataloader.dataset.classes\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    for i in range(n_images):\n",
    "        if i >= len(images): break\n",
    "        ax = plt.subplot(2, n_images // 2, i + 1)\n",
    "\n",
    "        # undo normalization and convert tensor to numpy image\n",
    "        img = images[i].numpy().transpose((1, 2, 0))\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img = std * img + mean\n",
    "        img = np.clip(img, 0, 1)\n",
    "\n",
    "        plt.imshow(img)\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.suptitle(\"Visualização de Imagens do DataLoader\", fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af4f525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displays train images\n",
    "show_images(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5de2446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displays validation images\n",
    "show_images(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e24a6c",
   "metadata": {},
   "source": [
    "## Funções Auxiliares "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cebd2042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model: nn.Module, train_loader: DataLoader, loss_fn, optimizer, device: torch.device) -> Tuple[float, List[int], List[int]]:\n",
    "  \"\"\"Trains the model over a dataset (training set).\"\"\"\n",
    "\n",
    "  model.train()\n",
    "  total_loss = 0.0\n",
    "  all_preds, all_labels = [], []\n",
    "\n",
    "  for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "    # move inputs and labels to the selected device\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "    # forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_fn(outputs, labels)\n",
    "\n",
    "    # backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # accumulate loss\n",
    "    total_loss += loss.cpu().item()\n",
    "\n",
    "    # store predictions and labels for metrics\n",
    "    all_labels.extend(labels.cpu().numpy())\n",
    "    all_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "\n",
    "  return total_loss / len(train_loader), all_labels, all_preds\n",
    "\n",
    "def evaluation_loop(model: nn.Module, loader: DataLoader, loss_fn, device: torch.device) -> Tuple[float, List[int], List[int]]:\n",
    "  \"\"\"Evaluates the model over a dataset (validation or test set).\"\"\"\n",
    "\n",
    "  model.eval()\n",
    "  total_loss = 0\n",
    "  all_preds, all_labels = [], []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for inputs, labels in tqdm(loader, desc=\"Evaluating\"):\n",
    "      # move inputs and labels to the selected device\n",
    "      inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "      # forward pass\n",
    "      outputs = model(inputs)\n",
    "      loss = loss_fn(outputs, labels)\n",
    "      total_loss += loss.item()\n",
    "\n",
    "      # store predictions and labels for metrics\n",
    "      preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "      all_preds.extend(preds)\n",
    "      all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # return average loss and collected predictions and labels\n",
    "    return total_loss / len(loader), all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edda6dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_failure_cases_from_results(dataset, y_true, y_pred, class_names, n_failures=15):\n",
    "    \"\"\"Finds and displays images from a dataset where the prediction was incorrect.\"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # find the indices where the prediction was wrong\n",
    "    mismatched_idx = np.where(y_true != y_pred)[0]\n",
    "    failures_to_show_idx = np.random.choice(mismatched_idx, size=min(n_failures, len(mismatched_idx)), replace=False)\n",
    "\n",
    "    # grid plotting logic\n",
    "    cols = 5\n",
    "    rows = int(np.ceil(len(failures_to_show_idx) / cols))\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(20, 3 * rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, idx in enumerate(failures_to_show_idx):\n",
    "        image_tensor, true_label_idx = dataset[idx]\n",
    "        pred_label_idx = y_pred[idx]\n",
    "\n",
    "        # reverts the normalization on a tensor image\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        tensor = image_tensor.clone().cpu().numpy().transpose((1, 2, 0))\n",
    "\n",
    "        img_to_show = std * tensor + mean\n",
    "        img_to_show = np.clip(img_to_show, 0, 1)\n",
    "\n",
    "        # plot the image\n",
    "        ax = axes[i]\n",
    "        ax.imshow(img_to_show)\n",
    "        ax.set_title(f\"True: {class_names[true_label_idx]}\\nPred: {class_names[pred_label_idx]}\", color='red', fontsize=10)\n",
    "        ax.axis('off')\n",
    "\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdbcf27",
   "metadata": {},
   "source": [
    "## CNN Customizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b65e13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custon CNN creation\n",
    "custom_cnn = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(16),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "\n",
    "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "\n",
    "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "\n",
    "    nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "\n",
    "    # classifier block\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=128 * 14 * 14, out_features=512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(in_features=512, out_features=len(class_names))\n",
    ")\n",
    "\n",
    "# determine device and move model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "custom_cnn = custom_cnn.to(device)\n",
    "\n",
    "# displays a summary of the custom CNN model\n",
    "summary(custom_cnn, input_size=(3, 224, 224), device=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac83236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(custom_cnn.parameters(), lr=1e-3)\n",
    "\n",
    "# metrics for early stopping\n",
    "patience = 5\n",
    "best_epoch = 0\n",
    "epochs_no_improve = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "for epoch in range(20):\n",
    "    # training step\n",
    "    train_loss, true_train, pred_train = train_loop(custom_cnn, train_loader, loss_fn, optimizer, device)\n",
    "    train_acc = accuracy_score(true_train, pred_train)\n",
    "\n",
    "    # validation step\n",
    "    val_loss, true_val, pred_val = evaluation_loop(custom_cnn, val_loader, loss_fn, device)\n",
    "    val_acc = accuracy_score(true_val, pred_val)\n",
    "\n",
    "    # store metrics\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_acc'].append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{20} Summary | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Epoch {epoch+1}/{20} Summary | Train Acc: {train_acc:.4f}  | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # early stopping verification\n",
    "    if val_loss < best_val_loss:\n",
    "        best_epoch = epoch\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(custom_cnn.state_dict(), f\"{MODELS_PATH}/best_custom_cnn.pth\")\n",
    "        print(f\"Validation loss improved. Saving model to best_custom_cnn.pth\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"Validation loss did not improve. Patience: {epochs_no_improve}/{patience}\")\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"\\nEarly stopping triggered after {patience} epochs with no improvement.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4310f8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# plot loss values\n",
    "ax1.plot(history['train_loss'], label='Loss de Treinamento', marker='o', color='red')\n",
    "ax1.plot(history['val_loss'], label='Loss de Validação', marker='o', color='blue')\n",
    "ax1.axvline(x=best_epoch, color='g', linestyle='--', label=f'Melhor Modelo (Época {best_epoch+1})')\n",
    "ax1.set_title(\"Evolução da Loss por Época\")\n",
    "ax1.set_xlabel('Épocas')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "\n",
    "# plot accuracy values\n",
    "ax2.plot(history['train_acc'], label='Acurácia de Treinamento', marker='o', color='red')\n",
    "ax2.plot(history['val_acc'], label='Acurácia de Validação', marker='o', color='blue')\n",
    "ax2.axvline(x=best_epoch, color='g', linestyle='--', label=f'Melhor Modelo (Época {best_epoch+1})')\n",
    "ax2.set_title('Evolução da Acurácia por Época')\n",
    "ax2.set_xlabel('Épocas')\n",
    "ax2.set_ylabel('Acurácia')\n",
    "ax2.legend()\n",
    "\n",
    "plt.savefig(f\"{CHARTS_PATH}/custom_cnn_training.svg\", format='svg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67ce4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displays the classification report\n",
    "custom_cnn.load_state_dict(torch.load(f\"{MODELS_PATH}/best_custom_cnn.pth\"))\n",
    "custom_cnn_test_loss, custom_cnn_true_test, custom_cnn_pred_test = evaluation_loop(custom_cnn, test_loader, loss_fn, device)\n",
    "print(classification_report(custom_cnn_true_test, custom_cnn_pred_test, target_names=class_names, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76bfcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ConfusionMatrixDisplay.from_predictions(y_true=custom_cnn_true_test, y_pred=custom_cnn_pred_test, cmap='Reds', display_labels=class_names, normalize='true', ax=ax)\n",
    "ax.set_title('CNN Customizada - Matriz de Confusão')\n",
    "ax.set_ylabel('Classe Real')\n",
    "ax.set_xlabel('Classe Prevista')\n",
    "\n",
    "plt.savefig(f\"{CHARTS_PATH}/custom_cnn_confusion_matrix.svg\", format='svg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445ff48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_failure_cases_from_results(test_dataset, custom_cnn_true_test, custom_cnn_pred_test, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d6866b",
   "metadata": {},
   "source": [
    "##  CNN Pré-Treinada com Camadas Convolucionais Congeladas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296fccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = models.resnet50(weights=\"IMAGENET1K_V2\")\n",
    "\n",
    "# freeze all layers to prevent them from being updated during training\n",
    "for param in pretrained_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# replace the final fc layer to match the number of classes in the dataset\n",
    "num_features = pretrained_model.fc.in_features\n",
    "pretrained_model.fc = nn.Linear(num_features, len(class_names))\n",
    "\n",
    "# determine device and move model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pretrained_model = pretrained_model.to(device)\n",
    "\n",
    "# displays a summary of the CNN model\n",
    "summary(pretrained_model, input_size=(3, 224, 224), device=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b798a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(pretrained_model.fc.parameters(), lr=1e-3)\n",
    "\n",
    "# metrics for early stopping\n",
    "patience = 5\n",
    "best_epoch = 0\n",
    "epochs_no_improve = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "for epoch in range(20):\n",
    "    # training step\n",
    "    train_loss, true_train, pred_train = train_loop(pretrained_model, train_loader, loss_fn, optimizer, device)\n",
    "    train_acc = accuracy_score(true_train, pred_train)\n",
    "\n",
    "    # validation step\n",
    "    val_loss, true_val, pred_val = evaluation_loop(pretrained_model, val_loader, loss_fn, device)\n",
    "    val_acc = accuracy_score(true_val, pred_val)\n",
    "\n",
    "    # store metrics\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_acc'].append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{20} Summary | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Epoch {epoch+1}/{20} Summary | Train Acc: {train_acc:.4f}  | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # early stopping verification\n",
    "    if val_loss < best_val_loss:\n",
    "        best_epoch = epoch\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(pretrained_model.state_dict(), f\"{MODELS_PATH}/best_frozen_pretrained_model.pth\")\n",
    "        print(f\"Validation loss improved. Saving model to best_frozen_pretrained_model.pth\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"Validation loss did not improve. Patience: {epochs_no_improve}/{patience}\")\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"\\nEarly stopping triggered after {patience} epochs with no improvement.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7f7696",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# plot loss values\n",
    "ax1.plot(history['train_loss'], label='Loss de Treinamento', marker='o', color='red')\n",
    "ax1.plot(history['val_loss'], label='Loss de Validação', marker='o', color='blue')\n",
    "ax1.axvline(x=best_epoch, color='g', linestyle='--', label=f'Melhor Modelo (Época {best_epoch+1})')\n",
    "ax1.set_title(\"Evolução da Loss por Época\")\n",
    "ax1.set_xlabel('Épocas')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "\n",
    "# plot accuracy values\n",
    "ax2.plot(history['train_acc'], label='Acurácia de Treinamento', marker='o', color='red')\n",
    "ax2.plot(history['val_acc'], label='Acurácia de Validação', marker='o', color='blue')\n",
    "ax2.axvline(x=best_epoch, color='g', linestyle='--', label=f'Melhor Modelo (Época {best_epoch+1})')\n",
    "ax2.set_title('Evolução da Acurácia por Época')\n",
    "ax2.set_xlabel('Épocas')\n",
    "ax2.set_ylabel('Acurácia')\n",
    "ax2.legend()\n",
    "\n",
    "plt.savefig(f\"{CHARTS_PATH}/frozen_pretained_model_training.svg\", format='svg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8b0613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displays the classification report\n",
    "pretrained_model.load_state_dict(torch.load(f\"{MODELS_PATH}/best_frozen_pretrained_model.pth\"))\n",
    "pretrained_model_test_loss, pretrained_model_true_test, pretrained_model_pred_test = evaluation_loop(pretrained_model, test_loader, loss_fn, device)\n",
    "print(classification_report(pretrained_model_true_test, pretrained_model_pred_test, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80f17b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ConfusionMatrixDisplay.from_predictions(y_true=pretrained_model_true_test, y_pred=pretrained_model_pred_test, cmap='Reds', display_labels=class_names, normalize='true', ax=ax)\n",
    "ax.set_title('CNN Pré-Treinada - Matriz de Confusão')\n",
    "ax.set_ylabel('Classe Real')\n",
    "ax.set_xlabel('Classe Prevista')\n",
    "\n",
    "plt.savefig(f\"{CHARTS_PATH}/pretrained_confusion_matrix.svg\", format='svg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d4b947",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_failure_cases_from_results(test_dataset, pretrained_model_true_test, pretrained_model_pred_test, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647cc1f5",
   "metadata": {},
   "source": [
    "## CNN Totalmente Pré-Treinada (*Fine-Tuning*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f908e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = models.resnet50(weights=\"IMAGENET1K_V2\")\n",
    "\n",
    "# replace the final fc layer to match the number of classes in the dataset\n",
    "num_features = finetuned_model.fc.in_features\n",
    "finetuned_model.fc = nn.Linear(num_features, len(class_names))\n",
    "\n",
    "# freeze all layers of the model\n",
    "for param in finetuned_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# unfreeze only the parameters of the new classifier head\n",
    "for param in finetuned_model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# determine device and move model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "finetuned_model = finetuned_model.to(device)\n",
    "\n",
    "# displays a summary of the CNN model\n",
    "summary(finetuned_model, input_size=(3, 224, 224), device=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d2bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize loss function and optimizer for the head\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer_head = optim.Adam(finetuned_model.fc.parameters(), lr=1e-3)\n",
    "\n",
    "# train the head for a few epochs to warm it up\n",
    "for epoch in range(5):\n",
    "    train_loss, _, _ = train_loop(finetuned_model, train_loader, loss_fn, optimizer_head, device)\n",
    "    val_loss, _, _ = evaluation_loop(finetuned_model, val_loader, loss_fn, device)\n",
    "    print(f\"Epoch [Head] {epoch+1}/5 | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a038b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfreeze all layers for fine-tuning\n",
    "for param in finetuned_model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# create optimizer with a low learning rate for the entire model\n",
    "optimizer_finetune = optim.Adam(finetuned_model.parameters(), lr=1e-5)\n",
    "\n",
    "# metrics for early stopping\n",
    "patience = 5\n",
    "best_epoch = 0\n",
    "epochs_no_improve = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "for epoch in range(20):\n",
    "    # training step\n",
    "    train_loss, true_train, pred_train = train_loop(finetuned_model, train_loader, loss_fn, optimizer_finetune, device)\n",
    "    train_acc = accuracy_score(true_train, pred_train)\n",
    "\n",
    "    # validation step\n",
    "    val_loss, true_val, pred_val = evaluation_loop(finetuned_model, val_loader, loss_fn, device)\n",
    "    val_acc = accuracy_score(true_val, pred_val)\n",
    "\n",
    "    # store metrics\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_acc'].append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{20} Summary | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Epoch {epoch+1}/{20} Summary | Train Acc: {train_acc:.4f}  | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # early stopping verification\n",
    "    if val_loss < best_val_loss:\n",
    "        best_epoch = epoch\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(finetuned_model.state_dict(), f\"{MODELS_PATH}/best_finetuned_model.pth\")\n",
    "        print(f\"Validation loss improved. Saving model to best_finetuned_model.pth\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"Validation loss did not improve. Patience: {epochs_no_improve}/{patience}\")\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"\\nEarly stopping triggered after {patience} epochs with no improvement.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb65a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# plot loss values\n",
    "ax1.plot(history['train_loss'], label='Loss de Treinamento', marker='o', color='red')\n",
    "ax1.plot(history['val_loss'], label='Loss de Validação', marker='o', color='blue')\n",
    "ax1.axvline(x=best_epoch, color='g', linestyle='--', label=f'Melhor Modelo (Época {best_epoch+1})')\n",
    "ax1.set_title(\"Evolução da Loss por Época\")\n",
    "ax1.set_xlabel('Épocas')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "\n",
    "# plot accuracy values\n",
    "ax2.plot(history['train_acc'], label='Acurácia de Treinamento', marker='o', color='red')\n",
    "ax2.plot(history['val_acc'], label='Acurácia de Validação', marker='o', color='blue')\n",
    "ax2.axvline(x=best_epoch, color='g', linestyle='--', label=f'Melhor Modelo (Época {best_epoch+1})')\n",
    "ax2.set_title('Evolução da Acurácia por Época')\n",
    "ax2.set_xlabel('Épocas')\n",
    "ax2.set_ylabel('Acurácia')\n",
    "ax2.legend()\n",
    "\n",
    "plt.savefig(f\"{CHARTS_PATH}/finetuned_model_training.svg\", format='svg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4b324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displays the classification report\n",
    "finetuned_model.load_state_dict(torch.load(f\"{MODELS_PATH}/best_finetuned_model.pth\"))\n",
    "finetuned_model_test_loss, finetuned_model_true_test, finetuned_model_pred_test = evaluation_loop(finetuned_model, test_loader, loss_fn, device)\n",
    "print(classification_report(finetuned_model_true_test, finetuned_model_pred_test, target_names=class_names, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad81f594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ConfusionMatrixDisplay.from_predictions(y_true=finetuned_model_true_test, y_pred=finetuned_model_pred_test, cmap='Reds', display_labels=class_names, normalize='true', ax=ax)\n",
    "ax.set_title('CNN com Fine-Tuning - Matriz de Confusão')\n",
    "ax.set_ylabel('Classe Real')\n",
    "ax.set_xlabel('Classe Prevista')\n",
    "\n",
    "plt.savefig(f\"{CHARTS_PATH}/finetuned_confusion_matrix.svg\", format='svg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6d2911",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_failure_cases_from_results(test_dataset, finetuned_model_true_test, finetuned_model_pred_test, class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
